{"cells":[{"cell_type":"markdown","metadata":{"id":"MMKjY7v377i-"},"source":["# Домашнее задание: натренировать модель!\n","\n","Ваше задание натренировать модель генерировать гороскопы.\n","\n","* __dataset:__ датасет гороскопов https://huggingface.co/datasets/dkagramanyan/horoscopes_ru\n","dataset = load_dataset(\"dkagramanyan/horoscopes_ru\")\n","* __short lines:__ берите только первые 512 символов\n","* __adapter type:__ используйте LoRA как выше __и хотя бы один из:__\n","   - экстра adapter на lm_head\n","   - экстра adapter на MLP components (mlp.*)\n","   - LoRA из peft по аналогии  \n","\n","PEFT:\n","1. Обучите модель (например, https://huggingface.co/IlyaGusev/saiga_mistral_7b_lora ) с помощью LoRA аналогично коду из ноутбука на датасете гороскопов, оберните слои attention.\n","\n","2. Обучите модель с помощью peft.LoraConfig.\n","\n","3. Сравните результаты на нескольких промптах.\n","\n","Решение в формате ссылки на Google Colaboratory необходимо выложить в соответствующее задание на учебном портале.\n","\n","Дедлайн сдачи: 29 февраля 23:59"]},{"cell_type":"markdown","metadata":{"id":"Gun-iX8Bdid0"},"source":["### ClearML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vKkvTX6drCj"},"outputs":[],"source":["!pip install clearml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOMo6_-gdlNa","tags":[]},"outputs":[],"source":["class InitClearml:\n","    def __init__(self, project_name: str, task_name: str):\n","        self.project_name = project_name\n","        self.task_name  = task_name\n","\n","    def init_task(self):\n","        self.clearml_available = False\n","        try:\n","            from clearml import Task\n","\n","            web_server = \"https://app.clear.ml\"  # @param {type:\"string\"}\n","            api_server = \"https://api.clear.ml\"  # @param {type:\"string\"}\n","            files_server = \"https://files.clear.ml\"  # @param {type:\"string\"}\n","            access_key = \"\"  # @param {type:\"string\"}\n","            secret_key = \"\"  # @param {type:\"string\"}\n","\n","            if access_key == \"\" or secret_key == \"\":\n","                raise Exception(\"Empty access_key or secret_key\")\n","\n","            Task.set_credentials(\n","                web_host=web_server,\n","                api_host=api_server,\n","                files_host=files_server,\n","                key=access_key,\n","                secret=secret_key,\n","            )\n","\n","            self.clearml_available = True\n","            return Task\n","\n","        except ImportError:\n","            return\n","        except Exception as e:\n","            print(e)\n","\n","    def init_experiment(self, Task):\n","        if self.clearml_available:\n","            task = Task.init(project_name=self.project_name, task_name=self.task_name)\n","            return task"]},{"cell_type":"markdown","metadata":{"id":"38td0Sw_du4L"},"source":["### Скачивание необходимых библиотек"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nw1Y_4ve8ZmY","outputId":"834903a3-89c6-4410-b780-924cb6e546c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting peft\n","  Downloading peft-0.8.2-py3-none-any.whl (183 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n","Collecting optimum\n","  Downloading optimum-1.17.1-py3-none-any.whl (407 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes==0.41.2.post2\n","  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Collecting coloredlogs (from optimum)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.12)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers) (3.20.3)\n","Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Installing collected packages: bitsandbytes, humanfriendly, dill, multiprocess, coloredlogs, accelerate, datasets, peft, optimum\n","Successfully installed accelerate-0.27.2 bitsandbytes-0.41.2.post2 coloredlogs-15.0.1 datasets-2.17.1 dill-0.3.8 humanfriendly-10.0 multiprocess-0.70.16 optimum-1.17.1 peft-0.8.2\n"]}],"source":["!pip install datasets peft transformers optimum bitsandbytes==0.41.2.post2 accelerate"]},{"cell_type":"markdown","metadata":{"id":"l4PCOnH0rP9-"},"source":["# Импорт библиотек"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76Hpa5JzVGUB","tags":[]},"outputs":[],"source":["from math import exp\n","import gc\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    DataCollatorForLanguageModeling,\n","    TrainingArguments,\n","    Trainer\n",")\n","import bitsandbytes as bnb\n","from transformers.trainer_pt_utils import get_parameter_names\n","from peft import LoraConfig, TaskType\n","from peft import get_peft_model"]},{"cell_type":"markdown","metadata":{"id":"kW38HdFErh5-"},"source":["# Основная часть"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJyUWX9_tnWG","tags":[]},"outputs":[],"source":["def tokenize(tokenizer, element, prompt=\"Гороскоп на завтра: \"):\n","    if isinstance(element[\"text\"], list):\n","        text = [prompt + s for s in element[\"text\"]]\n","    else:\n","        text = prompt + element[\"text\"]\n","\n","    outputs = tokenizer(\n","        text,\n","        truncation=True,\n","        max_length=512,\n","        padding=\"max_length\",\n","        return_tensors='pt',\n","        add_special_tokens=True\n","    )\n","    return outputs\n","\n","def generate(model, prefix=\"Гороскоп на завтра: \", text=\"\", **kwargs):\n","    model.eval()\n","    inputs = tokenizer(\n","        prefix + text,\n","        return_tensors='pt',\n","        add_special_tokens=True\n","    )\n","\n","    with torch.no_grad():\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        outputs_ids = model.generate(\n","            input_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            max_new_tokens=100,\n","            eos_token_id=50257,\n","            **kwargs\n","        )\n","\n","        return tokenizer.batch_decode(\n","            outputs_ids.detach().cpu().numpy(),\n","            skip_special_tokens=True\n","        )"]},{"cell_type":"markdown","metadata":{"id":"Cmn6jPbVrlwc"},"source":["## Данные"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gn-PD9QMrdHP","outputId":"71f5e739-6c52-43ec-b9bd-13f8f0fd427d","tags":[]},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['date', 'sign', 'text'],\n","        num_rows: 66501\n","    })\n","    test: Dataset({\n","        features: ['date', 'sign', 'text'],\n","        num_rows: 6976\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data = load_dataset(\"dkagramanyan/horoscopes_ru\")\n","data"]},{"cell_type":"markdown","metadata":{"id":"tjLznn79sz9n"},"source":["## Модель и токенайзер"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjzjL6V4s2G3","outputId":"b069cce1-b3f6-4acd-aa24-4a9f28dc1b18","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"markdown","metadata":{"id":"Iks82yDHEzrK"},"source":["### Токенизация данных"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbTjX3gKZ5gT","tags":[]},"outputs":[],"source":["train_ds = data[\"train\"].shuffle(seed=1).select(range(400)).map(\n","    lambda x: tokenize(tokenizer=tokenizer, element=x), batched=True, remove_columns=data[\"train\"].column_names\n",")\n","test_ds = data[\"test\"].shuffle(seed=1).select(range(100)).map(\n","    lambda x: tokenize(tokenizer=tokenizer, element=x), batched=True, remove_columns=data[\"test\"].column_names\n",")"]},{"cell_type":"markdown","metadata":{"id":"OOeNlnXNrsEq","tags":[]},"source":["## Часть 1\n","> Обучите модель с помощью LoRA аналогично коду из ноутбука на датасете гороскопов, оберните слои attention."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"t3UTTbXL-ghK"},"outputs":[],"source":["clearml_logging = InitClearml(\"FinTech VTB\", \"IlyaGusev/saiga2_7b_lora(custom_class_LoRA)_2\")\n","Task = clearml_logging.init_task()\n","task = clearml_logging.init_experiment(Task)\n","clearml_available = clearml_logging.clearml_available"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-KnW4Czl-ghK"},"outputs":[],"source":["logger = task.get_logger()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGKiEXDVVhdK","tags":[]},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7fc893a35ddd4ae08025df79dde83169","9469163bf1004d7893aa806377a08cb0","4acad45f20a14b5cb5afd2294ba7668b","79d33036b1144bd58810a51a404fe52c","6ee74c6d05bc46b2b1d9e74b70f06078","0e3d00346921493f9a1530903a510ab1","2b43643c83a14a53af49f269a295fd2d","8e8ca3324a194d3e93122003f9de9dc8","52c40ceb7527406cb8cbd63f1f8150c2","5b4ada21511c481282e9b3572fd67f53","1c5133b2b7e04b0187795e5e5b65c7ba","2995990bd3d34c138de41e13fcd0aaf3"]},"id":"bQOIAsk5V-xr","outputId":"b7a19aa8-e36c-407a-9929-eb7def369d2c","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2995990bd3d34c138de41e13fcd0aaf3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"name":"stdout","output_type":"stream","text":["2024-02-29 10:59:08,778 - clearml.model - INFO - Selected model id: 24e198bb75434e0bb7470248c00eb8cc\n","2024-02-29 10:59:26,289 - clearml.model - INFO - Selected model id: e1b30cc602d94777b2e9db8dd60fe524\n","2024-02-29 10:59:35,940 - clearml.model - INFO - Selected model id: 755908d6613942cb9a3e083e5472eb01\n"]},{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=4096, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=4096, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    offload_state_dict=True,\n","    low_cpu_mem_usage=True,\n","    load_in_4bit=True,\n","    torch_dtype=torch.float32\n",")\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w38IRDuxSyNa","outputId":"de697bce-08de-47a0-bd27-674c9982de7c","tags":[]},"outputs":[{"data":{"text/plain":["['Гороскоп на завтра: 2022-01-16\\n Unterscheidung: 100%\\n\\nВысокая температура, солнечный день, ветер с юга.\\n\\nВысокая температура, солнечный день, ветер с юга.\\n\\nВысокая температура, солнечный день, ветер с юга.\\n\\nВысокая температура, солнечный день, ветер с ю']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["test_init_model = generate(model, prefix=\"Гороскоп на завтра: \")\n","\n","test_init_model"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"TUqj35fz-ghL","outputId":"b5747a40-1412-430b-a3ef-238b1bbb0454"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Гороскоп на завтра: 2022-01-16\\n Unterscheidung: 100%\\n\\nВысокая температура, солнечный день, ветер с юга.\\n\\nВысокая температура, солнечный день, ветер с юга.\\n\\nВысокая температура, солнечный день, ветер с юга.\\n\\nВысокая температура, солнечный день, ветер с ю']\n"]}],"source":["if clearml_available:\n","    logger.report_text(test_init_model)"]},{"cell_type":"markdown","metadata":{"id":"Bn5qP2AcCXfh"},"source":["### LoRA"]},{"cell_type":"markdown","metadata":{"id":"U4vXoqNdCi1D"},"source":["#### Custom class LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBsP5IOZvsyM","tags":[]},"outputs":[],"source":["class LoRALayer(nn.Module):\n","    def __init__(self, module, rank: int, alpha: float):\n","        super().__init__()\n","        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n","        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n","        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n","        self.alpha = alpha\n","\n","    def forward(self, x):\n","        adapter_output = torch.matmul(x, self.adapter_A)\n","        adapter_output = torch.matmul(adapter_output, self.adapter_B)\n","        x = self.alpha * adapter_output\n","\n","        return x\n","\n","\n","class LinearWithLoRA(nn.Module):\n","    def __init__(self, linear, rank: int, alpha: float = 1):\n","        super().__init__()\n","        self.module = linear\n","        self.lora = LoRALayer(\n","            self.module, rank, alpha\n","        )\n","\n","    def forward(self, x):\n","        return self.module(x) + self.lora(x)"]},{"cell_type":"markdown","metadata":{"id":"jWWCke6vCncy"},"source":["#### Тест LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzG4XPF7yzhk","outputId":"bfdf7d94-44fb-4cf8-f010-f0f9f34e999b","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["All tests passed!\n"]}],"source":["test_linear = nn.Linear(128, 128)\n","test_linear.weight.data[...] = torch.eye(128)\n","test_adapter = LinearWithLoRA(test_linear, rank=8)\n","\n","assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1)\n","\n","test_adapter.lora.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n","test_adapter.lora.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n","test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n","\n","dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n","assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n","dummy_loss.backward()\n","assert all(w.grad is not None for w in [test_adapter.lora.adapter_A, test_adapter.lora.adapter_B]), \"some adapter weights have no grad\"\n","assert torch.allclose(test_adapter.lora.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n","assert torch.allclose(test_adapter.lora.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n","# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n","del dummy_loss, test_linear, test_adapter\n","print(\"All tests passed!\")"]},{"cell_type":"markdown","metadata":{"id":"roJtVe3mbfH6"},"source":["#### Подставляем LoRA в оригинальную нейросеть"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaGjGrUjbr3y","tags":[]},"outputs":[],"source":["from functools import partial\n","task_params = {}\n","lora_params = {}\n","\n","# logger\n","lora_params[\"rank\"] = 16\n","lora_params[\"alpha\"] = 1\n","lora_params[\"dropout\"] = None\n","lora_params[\"query\"] = True\n","lora_params[\"key\"] = True\n","lora_params[\"value\"] = True\n","lora_params[\"projection\"] = True\n","lora_params[\"mlp\"] = False\n","lora_params[\"head\"] = False\n","\n","layers = []\n","\n","assign_lora = partial(LinearWithLoRA, rank=lora_params[\"rank\"], alpha=lora_params[\"alpha\"])\n","\n","for layer in model.model.layers:\n","    if lora_params[\"query\"]:\n","        layer.self_attn.q_proj = assign_lora(layer.self_attn.q_proj)\n","    if lora_params[\"key\"]:\n","        layer.self_attn.k_proj = assign_lora(layer.self_attn.k_proj)\n","    if lora_params[\"value\"]:\n","        layer.self_attn.v_proj = assign_lora(layer.self_attn.v_proj)\n","    if lora_params[\"projection\"]:\n","        layer.self_attn.o_proj = assign_lora(layer.self_attn.o_proj)\n","    if lora_params[\"mlp\"]:\n","        layer.mlp.gate_proj = assign_lora(layer.mlp.gate_proj)\n","        layer.mlp.up_proj = assign_lora(layer.mlp.up_proj)\n","        layer.mlp.down_proj = assign_lora(layer.mlp.down_proj)\n","if lora_params[\"head\"]:\n","    model.lm_head = assign_lora(model.lm_head)\n","\n","task_params[\"LoRA settings\"] = lora_params\n","# if clearml_available:\n","#     task.connect(task_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDkG0kgPYskB","outputId":"84dc9e31-9c61-467a-c9ca-669b3688bd84","tags":[]},"outputs":[{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): LinearWithLoRA(\n","            (module): lora.Linear4bit(\n","              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=4096, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=4096, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (lora): LoRALayer()\n","          )\n","          (k_proj): LinearWithLoRA(\n","            (module): lora.Linear4bit(\n","              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=4096, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=4096, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (lora): LoRALayer()\n","          )\n","          (v_proj): LinearWithLoRA(\n","            (module): lora.Linear4bit(\n","              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=4096, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=4096, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (lora): LoRALayer()\n","          )\n","          (o_proj): LinearWithLoRA(\n","            (module): lora.Linear4bit(\n","              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.05, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=4096, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=4096, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (lora): LoRALayer()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n","          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woXP0UfsSwGr","outputId":"5979a13f-e188-4ab3-9dc2-4c6fbf854fbb","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Градиенты считаются\n"]}],"source":["batch = tokenizer(\"Это будет что-то секретное:\", return_tensors='pt', return_token_type_ids=False)\n","# test a single training step, make sure we get meaningful gradients\n","with torch.cuda.amp.autocast(dtype=torch.float32):\n","    out = model.forward(**batch)\n","    (out.logits.norm() / 100).backward()\n","\n","for i, module in enumerate(model.modules()):\n","    if isinstance(module, LoRALayer):\n","        assert module.adapter_B.grad is not None\n","        assert module.adapter_B.grad.norm().item() > 0\n","\n","model.zero_grad(set_to_none=True)\n","print(\"Градиенты считаются\")"]},{"cell_type":"markdown","metadata":{"id":"wFVA_0RZFbp1"},"source":["### Подготовка к обучению"]},{"cell_type":"markdown","metadata":{"id":"FAxHtIB7FemZ"},"source":["Коллатор"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAu-CD5EdASd","tags":[]},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"]},{"cell_type":"markdown","metadata":{"id":"u396RKnOFm4h"},"source":["Параметры обучения"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORyNh0PRc9Z0","tags":[]},"outputs":[],"source":["# Определение параметров обучения\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    logging_dir=\"./runs\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=10,\n","    per_device_eval_batch_size=2,\n","    save_steps=20,\n","    logging_steps=2,\n","    learning_rate=5e-3,\n","    warmup_steps=500,\n","    lr_scheduler_type=\"cosine\",\n","    save_total_limit=4,\n","    prediction_loss_only=True,\n","    fp16=True,\n","    save_safetensors=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"g0eEfw3cFsZ4"},"source":["Оптимайзер Adam 8 bit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYpVzTKZgqR-","tags":[]},"outputs":[],"source":["decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n","decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n","        \"weight_decay\": training_args.weight_decay,\n","    },\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","\n","optimizer_kwargs = {\n","    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n","    \"eps\": training_args.adam_epsilon,\n","}\n","optimizer_kwargs[\"lr\"] = training_args.learning_rate\n","\n","adam_bnb_optim = bnb.optim.Adam8bit(\n","    optimizer_grouped_parameters,\n","    betas=(training_args.adam_beta1, training_args.adam_beta2),\n","    eps=training_args.adam_epsilon,\n","    lr=training_args.learning_rate,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Ev6br48MF4ZC"},"source":["Посмотрим, сколько обучаемых параметров в NN. Есть вероятность, что все параметры не очень правильно посчитаны. (Мб надо надо умножить на 2 до того, как добавил свой слой LoRA)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhclgpYwiYzJ","outputId":"b1b31490-8079-4503-ab51-972e5a58a01a","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 16_777_216\t all params: 3_533_967_360\t trainable%: 0.004747416795609567\n"]}],"source":["trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","params = sum(p.numel() for p in model.parameters())\n","print(f\"trainable params: {trainable_params:_}\\t all params: {params:_}\\t trainable%: {trainable_params/params}\")"]},{"cell_type":"markdown","metadata":{"id":"1xXg5_JJGT78"},"source":["### Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"id":"BXmfNchJaFW_","outputId":"9ebbe642-1cab-4689-8dc8-d24c826d57dc","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-02-29 10:59:51,564 - clearml.Task - WARNING - Parameters must be of builtin type (Transformers/accelerator_config[AcceleratorConfig])\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [40/40 10:09, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>2</td>\n","      <td>2.278200</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.161800</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.166700</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.087900</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.010900</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.074100</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.968900</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.959200</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.886900</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.942300</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.917300</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.885600</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.829100</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.832400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.856400</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>1.806700</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>1.880900</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.799500</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.897400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.781000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["2024-02-29 11:00:53,995 - clearml - INFO - inf value encountered. Reporting it as '0.0'. Use clearml.Logger.set_reporting_inf_value to assign another value\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n","  warnings.warn(\n","/opt/conda/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=40, training_loss=1.951166146993637, metrics={'train_runtime': 628.5159, 'train_samples_per_second': 0.636, 'train_steps_per_second': 0.064, 'total_flos': 8160335521382400.0, 'train_loss': 1.951166146993637, 'epoch': 1.0})"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["model.config.use_cache = False\n","# model._hf_peft_config_loaded = True\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds,\n","    args=training_args,\n","    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n","    optimizers=(adam_bnb_optim, None)\n",")\n","# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"TcUpY3FO-ghR"},"source":["### Немного размышлений"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"JsAyJpPt-ghR"},"source":["Слои обучаются те, которые как-раз такие были вставлены"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"r8hh2pR2-ghR","outputId":"7ba0e653-e2e4-4002-c919-67b10111fe62"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable Layers:\n","model.layers.0.self_attn.q_proj.lora.adapter_A\n","model.layers.0.self_attn.q_proj.lora.adapter_B\n","model.layers.0.self_attn.k_proj.lora.adapter_A\n","model.layers.0.self_attn.k_proj.lora.adapter_B\n","model.layers.0.self_attn.v_proj.lora.adapter_A\n","model.layers.0.self_attn.v_proj.lora.adapter_B\n","model.layers.0.self_attn.o_proj.lora.adapter_A\n","model.layers.0.self_attn.o_proj.lora.adapter_B\n","model.layers.1.self_attn.q_proj.lora.adapter_A\n","model.layers.1.self_attn.q_proj.lora.adapter_B\n","model.layers.1.self_attn.k_proj.lora.adapter_A\n","model.layers.1.self_attn.k_proj.lora.adapter_B\n","model.layers.1.self_attn.v_proj.lora.adapter_A\n","model.layers.1.self_attn.v_proj.lora.adapter_B\n","model.layers.1.self_attn.o_proj.lora.adapter_A\n","model.layers.1.self_attn.o_proj.lora.adapter_B\n","model.layers.2.self_attn.q_proj.lora.adapter_A\n","model.layers.2.self_attn.q_proj.lora.adapter_B\n","model.layers.2.self_attn.k_proj.lora.adapter_A\n","model.layers.2.self_attn.k_proj.lora.adapter_B\n","model.layers.2.self_attn.v_proj.lora.adapter_A\n","model.layers.2.self_attn.v_proj.lora.adapter_B\n","model.layers.2.self_attn.o_proj.lora.adapter_A\n","model.layers.2.self_attn.o_proj.lora.adapter_B\n","model.layers.3.self_attn.q_proj.lora.adapter_A\n","model.layers.3.self_attn.q_proj.lora.adapter_B\n","model.layers.3.self_attn.k_proj.lora.adapter_A\n","model.layers.3.self_attn.k_proj.lora.adapter_B\n","model.layers.3.self_attn.v_proj.lora.adapter_A\n","model.layers.3.self_attn.v_proj.lora.adapter_B\n","model.layers.3.self_attn.o_proj.lora.adapter_A\n","model.layers.3.self_attn.o_proj.lora.adapter_B\n","model.layers.4.self_attn.q_proj.lora.adapter_A\n","model.layers.4.self_attn.q_proj.lora.adapter_B\n","model.layers.4.self_attn.k_proj.lora.adapter_A\n","model.layers.4.self_attn.k_proj.lora.adapter_B\n","model.layers.4.self_attn.v_proj.lora.adapter_A\n","model.layers.4.self_attn.v_proj.lora.adapter_B\n","model.layers.4.self_attn.o_proj.lora.adapter_A\n","model.layers.4.self_attn.o_proj.lora.adapter_B\n","model.layers.5.self_attn.q_proj.lora.adapter_A\n","model.layers.5.self_attn.q_proj.lora.adapter_B\n","model.layers.5.self_attn.k_proj.lora.adapter_A\n","model.layers.5.self_attn.k_proj.lora.adapter_B\n","model.layers.5.self_attn.v_proj.lora.adapter_A\n","model.layers.5.self_attn.v_proj.lora.adapter_B\n","model.layers.5.self_attn.o_proj.lora.adapter_A\n","model.layers.5.self_attn.o_proj.lora.adapter_B\n","model.layers.6.self_attn.q_proj.lora.adapter_A\n","model.layers.6.self_attn.q_proj.lora.adapter_B\n","model.layers.6.self_attn.k_proj.lora.adapter_A\n","model.layers.6.self_attn.k_proj.lora.adapter_B\n","model.layers.6.self_attn.v_proj.lora.adapter_A\n","model.layers.6.self_attn.v_proj.lora.adapter_B\n","model.layers.6.self_attn.o_proj.lora.adapter_A\n","model.layers.6.self_attn.o_proj.lora.adapter_B\n","model.layers.7.self_attn.q_proj.lora.adapter_A\n","model.layers.7.self_attn.q_proj.lora.adapter_B\n","model.layers.7.self_attn.k_proj.lora.adapter_A\n","model.layers.7.self_attn.k_proj.lora.adapter_B\n","model.layers.7.self_attn.v_proj.lora.adapter_A\n","model.layers.7.self_attn.v_proj.lora.adapter_B\n","model.layers.7.self_attn.o_proj.lora.adapter_A\n","model.layers.7.self_attn.o_proj.lora.adapter_B\n","model.layers.8.self_attn.q_proj.lora.adapter_A\n","model.layers.8.self_attn.q_proj.lora.adapter_B\n","model.layers.8.self_attn.k_proj.lora.adapter_A\n","model.layers.8.self_attn.k_proj.lora.adapter_B\n","model.layers.8.self_attn.v_proj.lora.adapter_A\n","model.layers.8.self_attn.v_proj.lora.adapter_B\n","model.layers.8.self_attn.o_proj.lora.adapter_A\n","model.layers.8.self_attn.o_proj.lora.adapter_B\n","model.layers.9.self_attn.q_proj.lora.adapter_A\n","model.layers.9.self_attn.q_proj.lora.adapter_B\n","model.layers.9.self_attn.k_proj.lora.adapter_A\n","model.layers.9.self_attn.k_proj.lora.adapter_B\n","model.layers.9.self_attn.v_proj.lora.adapter_A\n","model.layers.9.self_attn.v_proj.lora.adapter_B\n","model.layers.9.self_attn.o_proj.lora.adapter_A\n","model.layers.9.self_attn.o_proj.lora.adapter_B\n","model.layers.10.self_attn.q_proj.lora.adapter_A\n","model.layers.10.self_attn.q_proj.lora.adapter_B\n","model.layers.10.self_attn.k_proj.lora.adapter_A\n","model.layers.10.self_attn.k_proj.lora.adapter_B\n","model.layers.10.self_attn.v_proj.lora.adapter_A\n","model.layers.10.self_attn.v_proj.lora.adapter_B\n","model.layers.10.self_attn.o_proj.lora.adapter_A\n","model.layers.10.self_attn.o_proj.lora.adapter_B\n","model.layers.11.self_attn.q_proj.lora.adapter_A\n","model.layers.11.self_attn.q_proj.lora.adapter_B\n","model.layers.11.self_attn.k_proj.lora.adapter_A\n","model.layers.11.self_attn.k_proj.lora.adapter_B\n","model.layers.11.self_attn.v_proj.lora.adapter_A\n","model.layers.11.self_attn.v_proj.lora.adapter_B\n","model.layers.11.self_attn.o_proj.lora.adapter_A\n","model.layers.11.self_attn.o_proj.lora.adapter_B\n","model.layers.12.self_attn.q_proj.lora.adapter_A\n","model.layers.12.self_attn.q_proj.lora.adapter_B\n","model.layers.12.self_attn.k_proj.lora.adapter_A\n","model.layers.12.self_attn.k_proj.lora.adapter_B\n","model.layers.12.self_attn.v_proj.lora.adapter_A\n","model.layers.12.self_attn.v_proj.lora.adapter_B\n","model.layers.12.self_attn.o_proj.lora.adapter_A\n","model.layers.12.self_attn.o_proj.lora.adapter_B\n","model.layers.13.self_attn.q_proj.lora.adapter_A\n","model.layers.13.self_attn.q_proj.lora.adapter_B\n","model.layers.13.self_attn.k_proj.lora.adapter_A\n","model.layers.13.self_attn.k_proj.lora.adapter_B\n","model.layers.13.self_attn.v_proj.lora.adapter_A\n","model.layers.13.self_attn.v_proj.lora.adapter_B\n","model.layers.13.self_attn.o_proj.lora.adapter_A\n","model.layers.13.self_attn.o_proj.lora.adapter_B\n","model.layers.14.self_attn.q_proj.lora.adapter_A\n","model.layers.14.self_attn.q_proj.lora.adapter_B\n","model.layers.14.self_attn.k_proj.lora.adapter_A\n","model.layers.14.self_attn.k_proj.lora.adapter_B\n","model.layers.14.self_attn.v_proj.lora.adapter_A\n","model.layers.14.self_attn.v_proj.lora.adapter_B\n","model.layers.14.self_attn.o_proj.lora.adapter_A\n","model.layers.14.self_attn.o_proj.lora.adapter_B\n","model.layers.15.self_attn.q_proj.lora.adapter_A\n","model.layers.15.self_attn.q_proj.lora.adapter_B\n","model.layers.15.self_attn.k_proj.lora.adapter_A\n","model.layers.15.self_attn.k_proj.lora.adapter_B\n","model.layers.15.self_attn.v_proj.lora.adapter_A\n","model.layers.15.self_attn.v_proj.lora.adapter_B\n","model.layers.15.self_attn.o_proj.lora.adapter_A\n","model.layers.15.self_attn.o_proj.lora.adapter_B\n","model.layers.16.self_attn.q_proj.lora.adapter_A\n","model.layers.16.self_attn.q_proj.lora.adapter_B\n","model.layers.16.self_attn.k_proj.lora.adapter_A\n","model.layers.16.self_attn.k_proj.lora.adapter_B\n","model.layers.16.self_attn.v_proj.lora.adapter_A\n","model.layers.16.self_attn.v_proj.lora.adapter_B\n","model.layers.16.self_attn.o_proj.lora.adapter_A\n","model.layers.16.self_attn.o_proj.lora.adapter_B\n","model.layers.17.self_attn.q_proj.lora.adapter_A\n","model.layers.17.self_attn.q_proj.lora.adapter_B\n","model.layers.17.self_attn.k_proj.lora.adapter_A\n","model.layers.17.self_attn.k_proj.lora.adapter_B\n","model.layers.17.self_attn.v_proj.lora.adapter_A\n","model.layers.17.self_attn.v_proj.lora.adapter_B\n","model.layers.17.self_attn.o_proj.lora.adapter_A\n","model.layers.17.self_attn.o_proj.lora.adapter_B\n","model.layers.18.self_attn.q_proj.lora.adapter_A\n","model.layers.18.self_attn.q_proj.lora.adapter_B\n","model.layers.18.self_attn.k_proj.lora.adapter_A\n","model.layers.18.self_attn.k_proj.lora.adapter_B\n","model.layers.18.self_attn.v_proj.lora.adapter_A\n","model.layers.18.self_attn.v_proj.lora.adapter_B\n","model.layers.18.self_attn.o_proj.lora.adapter_A\n","model.layers.18.self_attn.o_proj.lora.adapter_B\n","model.layers.19.self_attn.q_proj.lora.adapter_A\n","model.layers.19.self_attn.q_proj.lora.adapter_B\n","model.layers.19.self_attn.k_proj.lora.adapter_A\n","model.layers.19.self_attn.k_proj.lora.adapter_B\n","model.layers.19.self_attn.v_proj.lora.adapter_A\n","model.layers.19.self_attn.v_proj.lora.adapter_B\n","model.layers.19.self_attn.o_proj.lora.adapter_A\n","model.layers.19.self_attn.o_proj.lora.adapter_B\n","model.layers.20.self_attn.q_proj.lora.adapter_A\n","model.layers.20.self_attn.q_proj.lora.adapter_B\n","model.layers.20.self_attn.k_proj.lora.adapter_A\n","model.layers.20.self_attn.k_proj.lora.adapter_B\n","model.layers.20.self_attn.v_proj.lora.adapter_A\n","model.layers.20.self_attn.v_proj.lora.adapter_B\n","model.layers.20.self_attn.o_proj.lora.adapter_A\n","model.layers.20.self_attn.o_proj.lora.adapter_B\n","model.layers.21.self_attn.q_proj.lora.adapter_A\n","model.layers.21.self_attn.q_proj.lora.adapter_B\n","model.layers.21.self_attn.k_proj.lora.adapter_A\n","model.layers.21.self_attn.k_proj.lora.adapter_B\n","model.layers.21.self_attn.v_proj.lora.adapter_A\n","model.layers.21.self_attn.v_proj.lora.adapter_B\n","model.layers.21.self_attn.o_proj.lora.adapter_A\n","model.layers.21.self_attn.o_proj.lora.adapter_B\n","model.layers.22.self_attn.q_proj.lora.adapter_A\n","model.layers.22.self_attn.q_proj.lora.adapter_B\n","model.layers.22.self_attn.k_proj.lora.adapter_A\n","model.layers.22.self_attn.k_proj.lora.adapter_B\n","model.layers.22.self_attn.v_proj.lora.adapter_A\n","model.layers.22.self_attn.v_proj.lora.adapter_B\n","model.layers.22.self_attn.o_proj.lora.adapter_A\n","model.layers.22.self_attn.o_proj.lora.adapter_B\n","model.layers.23.self_attn.q_proj.lora.adapter_A\n","model.layers.23.self_attn.q_proj.lora.adapter_B\n","model.layers.23.self_attn.k_proj.lora.adapter_A\n","model.layers.23.self_attn.k_proj.lora.adapter_B\n","model.layers.23.self_attn.v_proj.lora.adapter_A\n","model.layers.23.self_attn.v_proj.lora.adapter_B\n","model.layers.23.self_attn.o_proj.lora.adapter_A\n","model.layers.23.self_attn.o_proj.lora.adapter_B\n","model.layers.24.self_attn.q_proj.lora.adapter_A\n","model.layers.24.self_attn.q_proj.lora.adapter_B\n","model.layers.24.self_attn.k_proj.lora.adapter_A\n","model.layers.24.self_attn.k_proj.lora.adapter_B\n","model.layers.24.self_attn.v_proj.lora.adapter_A\n","model.layers.24.self_attn.v_proj.lora.adapter_B\n","model.layers.24.self_attn.o_proj.lora.adapter_A\n","model.layers.24.self_attn.o_proj.lora.adapter_B\n","model.layers.25.self_attn.q_proj.lora.adapter_A\n","model.layers.25.self_attn.q_proj.lora.adapter_B\n","model.layers.25.self_attn.k_proj.lora.adapter_A\n","model.layers.25.self_attn.k_proj.lora.adapter_B\n","model.layers.25.self_attn.v_proj.lora.adapter_A\n","model.layers.25.self_attn.v_proj.lora.adapter_B\n","model.layers.25.self_attn.o_proj.lora.adapter_A\n","model.layers.25.self_attn.o_proj.lora.adapter_B\n","model.layers.26.self_attn.q_proj.lora.adapter_A\n","model.layers.26.self_attn.q_proj.lora.adapter_B\n","model.layers.26.self_attn.k_proj.lora.adapter_A\n","model.layers.26.self_attn.k_proj.lora.adapter_B\n","model.layers.26.self_attn.v_proj.lora.adapter_A\n","model.layers.26.self_attn.v_proj.lora.adapter_B\n","model.layers.26.self_attn.o_proj.lora.adapter_A\n","model.layers.26.self_attn.o_proj.lora.adapter_B\n","model.layers.27.self_attn.q_proj.lora.adapter_A\n","model.layers.27.self_attn.q_proj.lora.adapter_B\n","model.layers.27.self_attn.k_proj.lora.adapter_A\n","model.layers.27.self_attn.k_proj.lora.adapter_B\n","model.layers.27.self_attn.v_proj.lora.adapter_A\n","model.layers.27.self_attn.v_proj.lora.adapter_B\n","model.layers.27.self_attn.o_proj.lora.adapter_A\n","model.layers.27.self_attn.o_proj.lora.adapter_B\n","model.layers.28.self_attn.q_proj.lora.adapter_A\n","model.layers.28.self_attn.q_proj.lora.adapter_B\n","model.layers.28.self_attn.k_proj.lora.adapter_A\n","model.layers.28.self_attn.k_proj.lora.adapter_B\n","model.layers.28.self_attn.v_proj.lora.adapter_A\n","model.layers.28.self_attn.v_proj.lora.adapter_B\n","model.layers.28.self_attn.o_proj.lora.adapter_A\n","model.layers.28.self_attn.o_proj.lora.adapter_B\n","model.layers.29.self_attn.q_proj.lora.adapter_A\n","model.layers.29.self_attn.q_proj.lora.adapter_B\n","model.layers.29.self_attn.k_proj.lora.adapter_A\n","model.layers.29.self_attn.k_proj.lora.adapter_B\n","model.layers.29.self_attn.v_proj.lora.adapter_A\n","model.layers.29.self_attn.v_proj.lora.adapter_B\n","model.layers.29.self_attn.o_proj.lora.adapter_A\n","model.layers.29.self_attn.o_proj.lora.adapter_B\n","model.layers.30.self_attn.q_proj.lora.adapter_A\n","model.layers.30.self_attn.q_proj.lora.adapter_B\n","model.layers.30.self_attn.k_proj.lora.adapter_A\n","model.layers.30.self_attn.k_proj.lora.adapter_B\n","model.layers.30.self_attn.v_proj.lora.adapter_A\n","model.layers.30.self_attn.v_proj.lora.adapter_B\n","model.layers.30.self_attn.o_proj.lora.adapter_A\n","model.layers.30.self_attn.o_proj.lora.adapter_B\n","model.layers.31.self_attn.q_proj.lora.adapter_A\n","model.layers.31.self_attn.q_proj.lora.adapter_B\n","model.layers.31.self_attn.k_proj.lora.adapter_A\n","model.layers.31.self_attn.k_proj.lora.adapter_B\n","model.layers.31.self_attn.v_proj.lora.adapter_A\n","model.layers.31.self_attn.v_proj.lora.adapter_B\n","model.layers.31.self_attn.o_proj.lora.adapter_A\n","model.layers.31.self_attn.o_proj.lora.adapter_B\n"]}],"source":["trainable_layers = []\n","\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        trainable_layers.append(name)\n","\n","print(\"Trainable Layers:\")\n","for layer in trainable_layers:\n","    print(layer)"]},{"cell_type":"markdown","metadata":{"id":"WgmQ270d-ghS"},"source":["Т.к. в чекпоинтах сохраняется названия других слоев, то можно их поменять их название, допуская то, что все-таки наши веса сохраняются.  \n","Проделываем все эти манипуляции и пытаемся снова перезапустить ноутбук и проинициализировать модель.  \n","В итоге получается, что *Trainer* сохранял веса от LoRA, которая уже до этого была заложена в модели.  \n","Получается, что надо писать свой кастомный класс, который будет сохранять эти веса при обучении, чтобы потом их можно было проинициализировать."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"0bts7go9-ghS"},"outputs":[],"source":["from itertools import product"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"98d75KX6-ghS"},"outputs":[],"source":["test_head = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n","lora_map = [\"A\", \"B\"]\n","layres_test = list(range(32))\n","\n","state_dict = torch.load(\"./results/checkpoint-100/adapter_model.bin\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"lKa508wo-ghS"},"outputs":[],"source":["all_combinations = list(product(test_head, lora_map, layres_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Un98_iL8-ghT","outputId":"ff68a965-29ee-4bcb-826b-c198ebb2c026"},"outputs":[{"data":{"text/plain":["(('q_proj', 'A', 0), ('q_proj', 'A', 31))"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["all_combinations[0], all_combinations[31]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"LTzDVJjj-ghT","outputId":"b7671c34-cda3-4955-9b62-18edc6282f2d"},"outputs":[{"data":{"text/plain":["(256, 256)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["len(all_combinations), len(state_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"6eTQf9gr-ghT","outputId":"b267ff38-f092-42fd-8ded-065ca9414411"},"outputs":[{"data":{"text/plain":["torch.Size([16, 4096])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["state_dict[\"base_model.model.model.layers.0.self_attn.q_proj.module.lora_A.weight\"].size()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"S3QhjdUw-ghU","outputId":"e48aa364-cc6b-4008-b74c-9a271cc877e0"},"outputs":[{"data":{"text/plain":["torch.Size([4096, 8])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["model.model.layers[0].self_attn.q_proj.lora.adapter_A.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"XzkRg_Th-ghU"},"outputs":[],"source":["param_mapping = {\n","    f\"base_model.model.model.layers.{l}.self_attn.{h}.module.lora_{a}.weight\": f\"model.layers.{l}.self_attn.{h}.lora.adapter_{a}\"\n","    for h, a, l in all_combinations\n","}\n","\n","# model.layers.0.self_attn.q_proj.lora.adapter_B"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"33HDxz9v-ghU","outputId":"5146bb01-e3cd-4f70-ff67-9b15b82d6324"},"outputs":[{"data":{"text/plain":["256"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["len(param_mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"abOjjZFT-ghV"},"outputs":[],"source":["mapped_state_dict = {}\n","for src_param_name, dst_param_name in param_mapping.items():\n","    if src_param_name in state_dict:\n","        mapped_state_dict[dst_param_name] = state_dict[src_param_name]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"onY24c_1-ghV"},"outputs":[],"source":["torch.save(mapped_state_dict, \"./results/checkpoint-100/adapter_model_fix.bin\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"AeaGDOBF-ghV","outputId":"87a0ff9d-18f6-4210-f01c-17126f9f7ce0"},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/checkpoint-100/adapter_model_fix.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.29.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.29.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.lora.adapter_A: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 8]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.lora.adapter_B: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([8, 4096])."]}],"source":["model.load_state_dict(torch.load(\"./results/checkpoint-100/adapter_model_fix.bin\"), strict=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"uQArmHPp-ghV","outputId":"e6590879-5d06-4727-8b7a-59b6fa6009fd"},"outputs":[{"data":{"text/plain":["Parameter containing:\n","tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["model.model.layers[0].self_attn.q_proj.lora.adapter_B"]},{"cell_type":"markdown","metadata":{"id":"tnkWwQopGraU"},"source":["### Eval Test (Perplexity)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"xIfuY2x0Gvte","outputId":"7df7eba3-dc6a-4111-a7e9-c3474c8683d9","tags":[]},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50/50 01:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 6.14\n"]}],"source":["eval_results = trainer.evaluate()\n","print(f\"Perplexity: {exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"Tafa3rtQQvSi"},"source":["### Некоторые промпты для этой модели"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4pmb1ErRfs6","outputId":"9b54ce78-afd1-4d06-a678-b79be482f365","tags":[]},"outputs":[{"data":{"text/plain":["['Гороскоп на завтра:  Сегодня вы можете ощутить недостаток внимания со стороны окружающих. В первую очередь это касается вашей семьи, друзей, партнеров по делу. Не исключены и недоразумения в отношениях с коллегами. Не стоит забывать, что ваша работа может быть не только важной, но и опасной. Не']"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["test_1_finetune_model_1 = generate(model)\n","\n","test_1_finetune_model_1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QvHPzAP6Rf5Z","outputId":"9f6a576f-5ee5-4de6-c32b-fadd80c84cd8","tags":[]},"outputs":[{"data":{"text/plain":["['Близнецам лучше провести день в домашних условиях, вдали от любопытства и внимания окружающих. Возможны споры и разногласия, но в целом это хороший день для семейных мелких поездок, покупки подарков, поиска новых интересных книг, покупки домашних приборов, покупки продуктов для домашнего приготовления. В этот']"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["test_2_finetune_model_1 = generate(model, text=\"Близнецам лучше провести день\", prefix=\"\")\n","\n","test_2_finetune_model_1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2A1mn7URgIK","outputId":"82de59c6-9f00-4dab-eddb-55ec3e12e3c9","tags":[]},"outputs":[{"data":{"text/plain":["['Гороскоп на завтра: Козерогам в этот день не стоит забывать о том, что любовь и сексуальные отношения могут привести к серьезным последствиям. Возможны разногласия с партнером по чувствам из-за различий во взглядах на семейную жизнь или воспитание детей. Не рекомендуется ухаживать за домашними животными - это может вызвать']"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["test_3_finetune_model_1 = generate(\n","    model,\n","    text=\"Козерогам в этот день\",\n","    do_sample=True,\n","    no_repeat_ngram_size=3,\n","    num_beams=8,\n","    top_p=0.7,\n","    top_k=100,\n","    repetition_penalty=5.0\n",")\n","\n","test_3_finetune_model_1"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"SNfW2n_s-ghX","outputId":"4357534f-900c-40b4-cd53-7dc2569e4795"},"outputs":[{"data":{"text/plain":["['Гороскоп на завтра: Весы сегодняшнего дня могут столкнуться с неожиданными препятствиями в своих планах. Возможно, вам придется пересмотреть свою стратегию или выбрать другой путь для достижения поставленной цели. Не исключен конфликт со своим партнером (в любом отношении). Постарайтесь избежа']"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["test_4_finetune_model_1 = generate(\n","    model,\n","    text=\"Весы\",\n","    do_sample=True,\n","    no_repeat_ngram_size=3,\n","    num_beams=8,\n","    top_p=0.7,\n","    top_k=100,\n","    repetition_penalty=5.0\n",")\n","\n","test_4_finetune_model_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugXw6rc9RgXK","tags":[],"outputId":"9e9939c0-c433-4c01-cbc8-f514787623a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Гороскоп на завтра:  Сегодня вы можете ощутить недостаток внимания со стороны окружающих. В первую очередь это касается вашей семьи, друзей, партнеров по делу. Не исключены и недоразумения в отношениях с коллегами. Не стоит забывать, что ваша работа может быть не только важной, но и опасной. Не']\n","['Близнецам лучше провести день в домашних условиях, вдали от любопытства и внимания окружающих. Возможны споры и разногласия, но в целом это хороший день для семейных мелких поездок, покупки подарков, поиска новых интересных книг, покупки домашних приборов, покупки продуктов для домашнего приготовления. В этот']\n","['Гороскоп на завтра: Козерогам в этот день не стоит забывать о том, что любовь и сексуальные отношения могут привести к серьезным последствиям. Возможны разногласия с партнером по чувствам из-за различий во взглядах на семейную жизнь или воспитание детей. Не рекомендуется ухаживать за домашними животными - это может вызвать']\n","['Гороскоп на завтра: Весы сегодняшнего дня могут столкнуться с неожиданными препятствиями в своих планах. Возможно, вам придется пересмотреть свою стратегию или выбрать другой путь для достижения поставленной цели. Не исключен конфликт со своим партнером (в любом отношении). Постарайтесь избежа']\n"]}],"source":["if clearml_available:\n","    logger.report_text(test_1_finetune_model_1)\n","    logger.report_text(test_2_finetune_model_1)\n","    logger.report_text(test_3_finetune_model_1)\n","    logger.report_text(test_4_finetune_model_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHzyBycIdvbn","tags":[]},"outputs":[],"source":["del model, trainer, training_args\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"7XjkaZ4pn93r"},"source":["## Часть 2\n","> Обучите модель с помощью peft.LoraConfig."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"3tu2g2Fw-ghY","outputId":"2500e805-1609-4108-9be4-97edc592dbcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["ClearML Task: created new task id=8734cccdb71b4809aa60df161cfc423f\n","2024-02-29 12:25:48,810 - clearml.Repository Detection - WARNING - Password protected Jupyter Notebook server was found! Add `sdk.development.jupyter_server_password=<jupyter_password>` to ~/clearml.conf\n","2024-02-29 12:25:48,815 - clearml.Task - INFO - Storing jupyter notebook directly as code\n","WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.\n","ClearML results page: https://app.clear.ml/projects/a0e39586066d49ac87ba40b5f2c1ec16/experiments/8734cccdb71b4809aa60df161cfc423f/output/log\n"]}],"source":["clearml_logging = InitClearml(\"FinTech VTB\", \"IlyaGusev/saiga2_7b_lora(default_class_LoRA)\")\n","Task = clearml_logging.init_task()\n","task = clearml_logging.init_experiment(Task)\n","clearml_available = clearml_logging.clearml_available"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"oEoGVHoT-ghZ"},"outputs":[],"source":["logger = task.get_logger()"]},{"cell_type":"markdown","metadata":{"id":"SNH9-kY7ZbVu"},"source":["### Конфиг LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYS9SJP6oCqD","tags":[]},"outputs":[],"source":["peft_config = LoraConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    inference_mode=False,\n","    r=16,\n","    lora_alpha=1,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"MxNRh3HXZhld"},"source":["### Модель"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["df28e134b40c400db90e10b9bb7f6601","2d1b17273c884753ba6df68af083cb03","b0e037019018475e9ef1ba1127e32ef9","9f8e0672dfa14d5f9cb463097b59601f","2f0c176259394dc684e60dfb9b01f834","51f54fc8c20f4d97869e9961fe956716","4235c29b9f3241bcb086cea4494d4c1c","a995a2c439084171bb3b00d1a14b0cfe","5275d5110c1e4c9da7b212bcd3428a8e","ea715c301d0f47cbadffa26bc531032c","7eb14a2ea6604cd79be93e691ae2a603","8191952af3b34346810aa94e7d41861e"]},"id":"6kD5h8LvWwZV","outputId":"6055b404-6ee0-49e6-ddbc-c6c7dc79675f","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8191952af3b34346810aa94e7d41861e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"name":"stdout","output_type":"stream","text":["2024-02-29 12:26:21,922 - clearml.model - INFO - Selected model id: 24e198bb75434e0bb7470248c00eb8cc\n","2024-02-29 12:26:41,638 - clearml.model - INFO - Selected model id: e1b30cc602d94777b2e9db8dd60fe524\n","2024-02-29 12:26:51,660 - clearml.model - INFO - Selected model id: 755908d6613942cb9a3e083e5472eb01\n"]}],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    offload_state_dict=True,\n","    low_cpu_mem_usage=True,\n","    load_in_4bit=True,\n","    torch_dtype=torch.float32\n",")\n","\n","# for param in model.parameters():\n","#     param.requires_grad = False\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"markdown","metadata":{"id":"6jhmu7xMZtj4"},"source":["### Подготовка к обучению"]},{"cell_type":"markdown","metadata":{"id":"UhLE9RmdaEVS"},"source":["Коллатор"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsdUTvuCaExw","tags":[]},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"]},{"cell_type":"markdown","metadata":{"id":"aRTNTdByaE9W"},"source":["Параметры обучения"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ik56TflaGwf","tags":[]},"outputs":[],"source":["# Определение параметров обучения\n","training_args = TrainingArguments(\n","    output_dir='./results_defLoRA',\n","    logging_dir=\"./runs\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=10,\n","    per_device_eval_batch_size=2,\n","    save_steps=20,\n","    logging_steps=2,\n","    learning_rate=5e-3,\n","    warmup_steps=500,\n","    lr_scheduler_type=\"cosine\",\n","    save_total_limit=4,\n","    prediction_loss_only=True,\n","    fp16=True,\n","    save_safetensors=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"NIxsDSlrZxIS"},"source":["Adam 8 bit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tS4rozTBY3Y5","tags":[]},"outputs":[],"source":["decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n","decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n","        \"weight_decay\": training_args.weight_decay,\n","    },\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","\n","optimizer_kwargs = {\n","    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n","    \"eps\": training_args.adam_epsilon,\n","}\n","optimizer_kwargs[\"lr\"] = training_args.learning_rate\n","\n","adam_bnb_optim = bnb.optim.Adam8bit(\n","    optimizer_grouped_parameters,\n","    betas=(training_args.adam_beta1, training_args.adam_beta2),\n","    eps=training_args.adam_epsilon,\n","    lr=training_args.learning_rate,\n",")"]},{"cell_type":"markdown","metadata":{"id":"4fuA-BtJaXAt"},"source":["Посмотрим, сколько обучающихся параметров"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dsfIyMuXEkw","outputId":"c67b5c6f-ed79-4286-85cf-04074f1c5805","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.24836028248556738\n"]}],"source":["model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":438},"id":"NH-G1cHsYsc3","outputId":"4ff29f84-201a-4362-a62e-4e0780a36e67","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-02-29 12:27:04,760 - clearml.Task - WARNING - Parameters must be of builtin type (Transformers/accelerator_config[AcceleratorConfig])\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:228: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [40/40 10:40, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>2</td>\n","      <td>2.325700</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.307800</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.282200</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.250500</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.266100</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.319500</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>2.253300</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>2.328300</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.259700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.255500</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>2.222600</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>2.166100</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>2.150800</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>2.116700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.995400</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>2.042400</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>2.059000</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.972500</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.965100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.980200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=40, training_loss=2.175976014137268, metrics={'train_runtime': 658.9761, 'train_samples_per_second': 0.607, 'train_steps_per_second': 0.061, 'total_flos': 8139719678361600.0, 'train_loss': 2.175976014137268, 'epoch': 1.0})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model.config.use_cache = False\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds,\n","    args=training_args,\n","    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n","    optimizers=(adam_bnb_optim, None)\n",")\n","# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"3867IS3Fah1O"},"source":["### Eval Test (Perplexity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCrQJ7mKaiz-","tags":[],"outputId":"22193873-6fc1-4657-e570-454c6b452f3c"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50/50 01:05]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 7.41\n"]}],"source":["eval_results = trainer.evaluate()\n","print(f\"Perplexity: {exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"HwSQCg0M-ghd"},"source":["### Некоторые промты для этой модели"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"g5fvNF1N-ghd","outputId":"885dcfe3-369f-4c6a-df6d-e686527eb0b5"},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"08JlP9dH-ghd","outputId":"9fd16867-feff-4074-9551-d614d568476a"},"outputs":[{"data":{"text/plain":["['Гороскоп на завтра: ♈ Январский солнечный день, который начинается с утреннего пробуждения, может быть очень активным и насыщенным. Hinweis: В этот день вы можете почувствовать себя особенно энергичным и уверенным в себе. Этот день может быть очень продуктивным для вас, если вы сможете использовать энергию, которую вы']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["test_1_finetune_model_2 = generate(model)\n","\n","test_1_finetune_model_2"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"YL4zueO0-ghe","outputId":"d4b9cc0b-4126-4aa0-cd1a-18d1e490ed25"},"outputs":[{"data":{"text/plain":["['Близнецам лучше провести день в доме, чем на улице. Hinweis: в детской игрушке нет ничего небезопасного.\\nВыбор игрушек для девочек: куклы, игрушки для маленьких девочек, игрушки для маленьких девочек, игрушки для маленьких девочек, игрушки для маленьких девочек, иг']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["test_2_finetune_model_2 = generate(model, text=\"Близнецам лучше провести день\", prefix=\"\")\n","\n","test_2_finetune_model_2"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"TW6Gbtxh-ghe","outputId":"0478324a-3117-4b24-b213-f7a9f51fb317"},"outputs":[{"data":{"text/plain":["['Гороскоп на завтра: Козерогам в этот день предстоит тяжёлое испытание, которое может привести к серьёзным последствиям для их карьеры и финансовой стабильности. февральские солнцепоклонники могут столкнуться с необходимостью выбирать между двумя противоречащими друг другу вариантами развития своих профессиональных или личных интересов']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["test_3_finetune_model_2 = generate(\n","    model,\n","    text=\"Козерогам в этот день\",\n","    do_sample=True,\n","    no_repeat_ngram_size=3,\n","    num_beams=8,\n","    top_p=0.7,\n","    top_k=100,\n","    repetition_penalty=5.0\n",")\n","\n","test_3_finetune_model_2"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"F8tAuJlz-ghe","outputId":"0ad0b0bf-96db-4eb1-8d5f-ac895d412967"},"outputs":[{"data":{"text/plain":["['Гороскоп на завтра: Весы вашего успеха сегодня весьма благоприятны, поэтому вы имеете все основания рассчитывать на хорошие результаты в любых сферах деятельности. prüfen Sie Ihren Horoskop jetzt kostenlos auf horoscope24.de!\\nПоследние 10 дней были очень напряженными и требовательными по отношению к Вашей энер']"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["test_4_finetune_model_2 = generate(\n","    model,\n","    text=\"Весы\",\n","    do_sample=True,\n","    no_repeat_ngram_size=3,\n","    num_beams=8,\n","    top_p=0.7,\n","    top_k=100,\n","    repetition_penalty=5.0\n",")\n","\n","test_4_finetune_model_2"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"sIL-9WhD-ghe","outputId":"bd7953a2-0380-418a-e8aa-e840ac03a2ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Гороскоп на завтра: ♈ Январский солнечный день, который начинается с утреннего пробуждения, может быть очень активным и насыщенным. Hinweis: В этот день вы можете почувствовать себя особенно энергичным и уверенным в себе. Этот день может быть очень продуктивным для вас, если вы сможете использовать энергию, которую вы']\n","['Близнецам лучше провести день в доме, чем на улице. Hinweis: в детской игрушке нет ничего небезопасного.\\nВыбор игрушек для девочек: куклы, игрушки для маленьких девочек, игрушки для маленьких девочек, игрушки для маленьких девочек, игрушки для маленьких девочек, иг']\n","['Гороскоп на завтра: Козерогам в этот день предстоит тяжёлое испытание, которое может привести к серьёзным последствиям для их карьеры и финансовой стабильности. февральские солнцепоклонники могут столкнуться с необходимостью выбирать между двумя противоречащими друг другу вариантами развития своих профессиональных или личных интересов']\n","['Гороскоп на завтра: Весы вашего успеха сегодня весьма благоприятны, поэтому вы имеете все основания рассчитывать на хорошие результаты в любых сферах деятельности. prüfen Sie Ihren Horoskop jetzt kostenlos auf horoscope24.de!\\nПоследние 10 дней были очень напряженными и требовательными по отношению к Вашей энер']\n"]}],"source":["if clearml_available:\n","    logger.report_text(test_1_finetune_model_2)\n","    logger.report_text(test_2_finetune_model_2)\n","    logger.report_text(test_3_finetune_model_2)\n","    logger.report_text(test_4_finetune_model_2)\n","task.close()"]},{"cell_type":"markdown","metadata":{"id":"1-BL1GEW-ghf"},"source":["# Выводы"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"JgLtCSpo-ghf","executionInfo":{"status":"ok","timestamp":1709212490376,"user_tz":-180,"elapsed":334,"user":{"displayName":"Nikita Selivyorstov","userId":"10058639862806756818"}},"outputId":"bbf83d09-f9c7-477d-c1c2-098b87aeb0ed"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table style=\"border:1px solid black\" >\n","  <tr>\n","    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n","    <th style=\"text-align: center; border:1px solid black\">FINETUNING CUSTOM LoRA</th>\n","    <th style=\"text-align: center; border:1px solid black\">FINETUNING DEFAULT LoRA</th>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Гороскоп на завтра:`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Гороскоп на завтра:  Сегодня вы можете ощутить недостаток внимания со стороны окружающих. \n"," В первую очередь это касается вашей семьи, друзей, партнеров по делу. \n"," Не исключены и недоразумения в отношениях с коллегами. \n"," Не стоит забывать, что ваша работа может быть не только важной, но и опасной. Не</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Гороскоп на завтра: ♈ Январский солнечный день, который начинается с утреннего пробуждения, \n","может быть очень активным и насыщенным. Hinweis: В этот день вы можете почувствовать себя особенно энергичным и уверенным в себе. \n"," Этот день может быть очень продуктивным для вас, если вы сможете использовать энергию, которую вы</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Близнецам лучше провести день`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Близнецам лучше провести день в домашних условиях, вдали от любопытства и внимания окружающих. \n"," Возможны споры и разногласия, но в целом это хороший день для семейных мелких поездок, покупки подарков, поиска новых интересных книг, \n","покупки домашних приборов, покупки продуктов для домашнего приготовления. В этот</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Близнецам лучше провести день в доме, чем на улице. \n"," Hinweis: в детской игрушке нет ничего небезопасного.\n","Выбор игрушек для девочек: куклы, игрушки для маленьких девочек, игрушки для маленьких девочек, \n"," игрушки для маленьких девочек, игрушки для маленьких девочек, иг</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Гороскоп на завтра: Козерогам в этот день`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Гороскоп на завтра: Козерогам в этот день не стоит забывать о том, что любовь и сексуальные отношения могут привести к серьезным последствиям. \n"," Возможны разногласия с партнером по чувствам из-за различий во взглядах на семейную жизнь \n"," или воспитание детей. Не рекомендуется ухаживать за домашними животными - это может вызвать</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Гороскоп на завтра: Козерогам в этот день предстоит тяжёлое испытание, \n"," которое может привести к серьёзным последствиям для их карьеры и финансовой стабильности. февральские солнцепоклонники могут \n"," столкнуться с необходимостью выбирать между двумя противоречащими друг другу вариантами развития \n"," своих профессиональных или личных интересов</pre></td>\n","  </tr>\n","  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`Гороскоп на завтра: Весы`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Гороскоп на завтра: Весы сегодняшнего дня могут столкнуться с неожиданными препятствиями в своих планах. \n"," Возможно, вам придется пересмотреть свою стратегию или выбрать другой путь для достижения поставленной цели. Не исключен конфликт со своим партнером (в любом отношении). Постарайтесь избежа</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Гороскоп на завтра: Весы вашего успеха сегодня весьма благоприятны, поэтому вы имеете \n"," все основания рассчитывать на хорошие результаты в любых сферах деятельности. prüfen Sie Ihren Horoskop jetzt kostenlos auf horoscope24.de! \n"," Последние 10 дней были очень напряженными и требовательными по отношению к Вашей энер</pre></td>\n","  </tr>\n","</table>"]},"metadata":{}}],"source":["from IPython.display import HTML, display\n","table_template = \"\"\"<table style=\"border:1px solid black\" >\n","  <tr>\n","    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n","    <th style=\"text-align: center; border:1px solid black\">FINETUNING CUSTOM LoRA</th>\n","    <th style=\"text-align: center; border:1px solid black\">FINETUNING DEFAULT LoRA</th>\n","  </tr>\n","{}\n","</table>\"\"\"\n","\n","row_template = '''  <tr>\n","    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n","    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n","  </tr>'''\n","\n","rows = []\n","\n","asnwers_custom_lora = [\n","    \"Гороскоп на завтра:  Сегодня вы можете ощутить недостаток внимания со стороны окружающих. \\n В первую очередь это касается вашей семьи, друзей, партнеров по делу. \\n Не исключены и недоразумения в отношениях с коллегами. \\n Не стоит забывать, что ваша работа может быть не только важной, но и опасной. Не\",\n","    \"Близнецам лучше провести день в домашних условиях, вдали от любопытства и внимания окружающих. \\n Возможны споры и разногласия, но в целом это хороший день для семейных мелких поездок, покупки подарков, поиска новых интересных книг, \\nпокупки домашних приборов, покупки продуктов для домашнего приготовления. В этот\",\n","    \"Гороскоп на завтра: Козерогам в этот день не стоит забывать о том, что любовь и сексуальные отношения могут привести к серьезным последствиям. \\n Возможны разногласия с партнером по чувствам из-за различий во взглядах на семейную жизнь \\n или воспитание детей. Не рекомендуется ухаживать за домашними животными - это может вызвать\",\n","    \"Гороскоп на завтра: Весы сегодняшнего дня могут столкнуться с неожиданными препятствиями в своих планах. \\n Возможно, вам придется пересмотреть свою стратегию или выбрать другой путь для достижения поставленной цели. Не исключен конфликт со своим партнером (в любом отношении). Постарайтесь избежа\"\n","]\n","\n","asnwers_default_lora = [\n","    \"Гороскоп на завтра: ♈ Январский солнечный день, который начинается с утреннего пробуждения, \\nможет быть очень активным и насыщенным. Hinweis: В этот день вы можете почувствовать себя особенно энергичным и уверенным в себе. \\n Этот день может быть очень продуктивным для вас, если вы сможете использовать энергию, которую вы\",\n","    \"Близнецам лучше провести день в доме, чем на улице. \\n Hinweis: в детской игрушке нет ничего небезопасного.\\nВыбор игрушек для девочек: куклы, игрушки для маленьких девочек, игрушки для маленьких девочек, \\n игрушки для маленьких девочек, игрушки для маленьких девочек, иг\",\n","    \"Гороскоп на завтра: Козерогам в этот день предстоит тяжёлое испытание, \\n которое может привести к серьёзным последствиям для их карьеры и финансовой стабильности. февральские солнцепоклонники могут \\n столкнуться с необходимостью выбирать между двумя противоречащими друг другу вариантами развития \\n своих профессиональных или личных интересов\",\n","    \"Гороскоп на завтра: Весы вашего успеха сегодня весьма благоприятны, поэтому вы имеете \\n все основания рассчитывать на хорошие результаты в любых сферах деятельности. prüfen Sie Ihren Horoskop jetzt kostenlos auf horoscope24.de! \\n Последние 10 дней были очень напряженными и требовательными по отношению к Вашей энер\"\n","]\n","\n","promts = ['Гороскоп на завтра:', 'Близнецам лучше провести день', 'Гороскоп на завтра: Козерогам в этот день', 'Гороскоп на завтра: Весы']\n","\n","for prompt, asnwer_custom_lora, asnwer_default_lora in zip(promts, asnwers_custom_lora, asnwers_default_lora):\n","    # replace placeholders in the format() arguments\n","    rows.append(row_template.format(prompt, asnwer_custom_lora, asnwer_default_lora))\n","\n","display(HTML(table_template.format('\\n'.join(rows))))"]},{"cell_type":"markdown","metadata":{"id":"FewcMQcL-ghf"},"source":["https://app.clear.ml/projects/a0e39586066d49ac87ba40b5f2c1ec16/experiments/17949fa0a99942738ae562a57b29013b/output/execution  \n","https://app.clear.ml/projects/a0e39586066d49ac87ba40b5f2c1ec16/experiments/8734cccdb71b4809aa60df161cfc423f/output/execution  "]},{"cell_type":"markdown","metadata":{"tags":[],"id":"MpEEDUXU-ghf"},"source":["Для второй модели, наверное, можно было дать больше данных, чтобы ответы были более вариативные. Учится более плавно, чем первая модель по лоссу.  \n","Для первой модели с увеличением данных градиентам становится \"плохо\". Скорей всего, можно было добавить dropout.  \n","При прочих равных у первой ответы чуть лучше, но опять же predict также надо преднастраивать для каждой модели.  "]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["TcUpY3FO-ghR"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0e3d00346921493f9a1530903a510ab1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c5133b2b7e04b0187795e5e5b65c7ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b43643c83a14a53af49f269a295fd2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d1b17273c884753ba6df68af083cb03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51f54fc8c20f4d97869e9961fe956716","placeholder":"​","style":"IPY_MODEL_4235c29b9f3241bcb086cea4494d4c1c","value":"Loading checkpoint shards: 100%"}},"2f0c176259394dc684e60dfb9b01f834":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4235c29b9f3241bcb086cea4494d4c1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4acad45f20a14b5cb5afd2294ba7668b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e8ca3324a194d3e93122003f9de9dc8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52c40ceb7527406cb8cbd63f1f8150c2","value":2}},"51f54fc8c20f4d97869e9961fe956716":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5275d5110c1e4c9da7b212bcd3428a8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52c40ceb7527406cb8cbd63f1f8150c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b4ada21511c481282e9b3572fd67f53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ee74c6d05bc46b2b1d9e74b70f06078":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79d33036b1144bd58810a51a404fe52c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b4ada21511c481282e9b3572fd67f53","placeholder":"​","style":"IPY_MODEL_1c5133b2b7e04b0187795e5e5b65c7ba","value":" 2/2 [01:18&lt;00:00, 36.09s/it]"}},"7eb14a2ea6604cd79be93e691ae2a603":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fc893a35ddd4ae08025df79dde83169":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9469163bf1004d7893aa806377a08cb0","IPY_MODEL_4acad45f20a14b5cb5afd2294ba7668b","IPY_MODEL_79d33036b1144bd58810a51a404fe52c"],"layout":"IPY_MODEL_6ee74c6d05bc46b2b1d9e74b70f06078"}},"8e8ca3324a194d3e93122003f9de9dc8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9469163bf1004d7893aa806377a08cb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e3d00346921493f9a1530903a510ab1","placeholder":"​","style":"IPY_MODEL_2b43643c83a14a53af49f269a295fd2d","value":"Loading checkpoint shards: 100%"}},"9f8e0672dfa14d5f9cb463097b59601f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea715c301d0f47cbadffa26bc531032c","placeholder":"​","style":"IPY_MODEL_7eb14a2ea6604cd79be93e691ae2a603","value":" 2/2 [01:16&lt;00:00, 35.02s/it]"}},"a995a2c439084171bb3b00d1a14b0cfe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0e037019018475e9ef1ba1127e32ef9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a995a2c439084171bb3b00d1a14b0cfe","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5275d5110c1e4c9da7b212bcd3428a8e","value":2}},"df28e134b40c400db90e10b9bb7f6601":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d1b17273c884753ba6df68af083cb03","IPY_MODEL_b0e037019018475e9ef1ba1127e32ef9","IPY_MODEL_9f8e0672dfa14d5f9cb463097b59601f"],"layout":"IPY_MODEL_2f0c176259394dc684e60dfb9b01f834"}},"ea715c301d0f47cbadffa26bc531032c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}